<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
    <style>
body {
  font-family: Arial, Helvetica, sans-serif;
  font-size: 20px;
}

#myBtn {
  display: none;
  position: fixed;
  bottom: 20px;
  right: 30px;
  z-index: 99;
  font-size: 18px;
  border: none;
  outline: none;
  background-color: #8d93ab;
  color: white;
  cursor: pointer;
  padding: 15px;
  border-radius: 4px;
}

#myBtn:hover {
  background-color: #555;
}
</style>
    <link rel="shortcut icon" type="image/png" href="pic/Q.png">
    <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
    <meta name="keywords" content="Shenglai Zeng, Jack Wang, Wang Jiaqi, IST, CS, PSU, ZJU, Zhejiang University, Data Science, Security, Machine Learning, Privacy, Optimization"> 
    <meta name="description" content="Shenglai Zengg's Homepage">
    <link rel="stylesheet" href="jemdoc.css" type="text/css" />
    <title>Shenglai Zeng</title>
    <script type="text/javascript">

      var _gaq = _gaq || [];
      _gaq.push(['_setAccount', 'UA-39824124-1']);
      _gaq.push(['_trackPageview']);

      (function() {
        var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
        ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
      })();

    </script>
    <script type="text/javascript">
       function toggle_visibility(id) {
           var e = document.getElementById(id);
           if(e.style.display == 'block')
              e.style.display = 'none';
           else
              e.style.display = 'block';
       }
    </script>
</head>

<body>
    <button onclick="topFunction()" id="myBtn" title="Go to top">Top</button>
    <script>
    //Get the button
    var mybutton = document.getElementById("myBtn");
    
    // When the user scrolls down 20px from the top of the document, show the button
    window.onscroll = function() {scrollFunction()};
    
    function scrollFunction() {
      if (document.body.scrollTop > 20 || document.documentElement.scrollTop > 20) {
        mybutton.style.display = "block";
      } else {
        mybutton.style.display = "none";
      }
    }
    
    // When the user clicks on the button, scroll to the top of the document
    function topFunction() {
      document.body.scrollTop = 0;
      document.documentElement.scrollTop = 0;
    }
    </script>













    
<nav class="navbar navbar-dark navbar-expand-lg fixed-top">
    <div id="layout-menu">
    <a href="#">Home</a>
        <a href="#About Me">About Me</a>
    <a href="#publications">Publications</a>
	<a href="#experience">Experience</a>
    <a href="#projects">Projects</a>
        <!--<a href="#publications">Products</a> -->
    <a href="#service">Service</a>

<!--    <a href="#service">Activities</a>-->
<!--    <a href="#teaching">Teaching</a>-->
        <a href="./Shenglai_CV.pdf">CV</a>
	<!-- <a href='#personal'>Personal</a> -->
    </div>
</nav>


<div id="layout-content" style="margin-top:25px">


<table>
	<tbody>
		<tr>
			<td width="670">
				<div id="toptitle">					
					<h1>Shenglai Zeng<h1>
				</div>

				<h3> Undergraduate Student</h3>
				<p>
					Yingcai Honor School </br>
                    University of Electronic Science and Technology of China </br>
		
					Email: shenglaizeng [at] gmail [dot] com
				</p>
				<p>
					<!--<a href="https://scholar.google.com/citations?user=ZGTVInsAAAAJ&hl=en"><img src="pic/GoogleScholar.png" height="30px" style="margin-bottom:-3px"></a> -->
                    <!-- <a href="https://www.linkedin.com/in/uniquexinwang"><img src="pic/LinkedIn.png" height="30px" style="margin-bottom:-3px"></a> -->
                    <!--<a href="https://github.com/eric-xw"><img src="pic/git.svg" height="30px" style="margin-bottom:-3px"></a> -->
                    <!--<iframe id="twitter-widget-4" scrolling="no" frameborder="0" allowtransparency="true" class="twitter-follow-button twitter-follow-button-rendered" title="Twitter Follow Button" src="https://platform.twitter.com/widgets/follow_button.59df888963e9d6219f6e6c7ff5d8b951.en.html#dnt=false&amp;id=twitter-widget-4&amp;lang=en&amp;screen_name=xwang_lk&amp;show_count=false&amp;show_screen_name=true&amp;size=m&amp;time=1524631465785" style="position: static; visibility: visible; width: 155px; height: 20px;" data-screen-name="TwitterDev"></iframe> -->
				</p>
			</td>
			<td>
                <!-- <img alt="" src="images/1.jpg" onMouseOver="this.src='pic/jiaqi.png'" onMouseOut="this.src='pic/Q.png'"> -->
                <!--<img class="img-circle" src="pic/jiaqi.png" border="0" width="200px"></br> -->
                <!-- <script language=javascript>
                    id=Math.round(Math.random()*3)+1
                    document.write( "<img class=\"img-circle\" src=\""+id+".png\" order=\"0\" width=\"200px\"></br>")
                </script> -->
                <img class="img-circle" alt="" src="pic/Shenglai.jpg" onMouseOver="this.src='pic/full.png'" onMouseOut="this.src='pic/Shenglai.jpg'" border="0" width="250px">
			</td>
		<tr>
	</tbody>
</table>

<br>

<div id="About Me">
<h2>About Me</h2>
<p style="text-align:justify; text-justify:inter-ideograph;"> I am an undergraduate student in the  <a href = "https://www.yingcai.uestc.edu.cn/">Yingcai Honor School</a>  at <a href = "http://www.uestc.edu.cn/"> University of Electronic Science and Technology of China</a>
, supervised by <a href = "https://scholar.google.com/citations?user=GmEdMqwAAAAJ&hl=en"> Professor Hongfang Yu</a>. I am also an intern of  <a href="https://ist.psu.edu/"> Peen state University</a> advised by <a href="http://www.personal.psu.edu/ffm5105/index.html/"> Professor Fenglong Ma</a> and  <a href = "https://www.ubc.ca
"> The University of British Columbia</a> advised by <a href="https://xxlya.github.io/xiaoxiao/"> Professor Xiaoxiao Li </a>. I am looking for a PhD position in 23-Fall, please don't hesitant to contact me if you are interested. 
 
<!-- <br><br> -->

<h3>Research Interests</h3>
<p style="text-align:justify; text-justify:inter-ideograph;">My research interests are mainly about Federated Learning/Distributed Machine Learning, Intelligent Network, and Internet-of-Things</p>	

<ul>
    <li>
        The optimization of federated learning(e.g. heterogenous FL, communication-efficient FL)
    </li>
    <li>
        Trustworthy AI/AI security(e.g. mathine unlearning)
    </li>
    <li>
       Applicable Machine Learning System Design for real-world scenario.(e.g.healthcare,IoT, Metaverse)   
    </li>
	<li>
       I would also like to try some new topics in the future, such as semantic communication,game theory and so on  
    </li>
	
</ul>

<!--<h3>Hiring</h3> -->
<!--<font color="darkred">I am looking for highly-motivated students (esp. PhD students starting in Fall 2021). 
    Please read <a href="./hiring.html">the information for <b>prospective students and visitors</b></a> and check out the most beautiful and unique campus of UCSC [<a href="https://www.youtube.com/watch?v=ogGSFZUjrwE">YouTube video</a> | <a href="https://www.bilibili.com/video/BV1xz411e77n?from=search&seid=1387866364580672367">bilibili video</a>]. </font> -->
<!-- <br><br> -->

<!-- <br><br>
His research interests include natural language processing, computer vision, and machine learning, especially the intersection of them. 
More specifically, 
<ul>
    <li>
        Language, Vision, and Knowledge,
    </li>
    <li>
        Embodied Agents,
    </li>
    <li>
        Multilingual Language Grounding,
    </li>
    <li>
        Human Activity Understanding in Videos.
    </li>
</ul> -->
</div>
<br>
<div id="news">
<h2>News</h2>
<div style="overflow-y: scroll; height:250px;">
<table>
        <!-- <tr valign="top"> <td>[<b>NEW!</b>] </td> <td> I successfully defended my Ph.D. Thesis "Closing the Loop Between Language and Vision for Embodied Agents". Thanks to the committee and everyone who has helped me along the Ph.D. journey!</td></tr>
        <tr valign="top"> <td>[<b>NEW!</b>] </td> <td> I am serving as an Area Chair for <a href="https://2020.emnlp.org">EMNLP 2020</a>. </td></tr>
        <tr valign="top"> <td>[<b>NEW!</b>] </td> <td> Organizing the <a href="https://alvr-workshop.github.io">workshop on Advances in Language and Vision Research (ALVR)</a> at ACL 2020. Welcome to participate on <b>July 9th</b>!</td></tr>
        <tr valign="top"> <td>[03/2020] </td> <td> Invited panelist at the <a href="https://www.nvidia.com/en-us/gtc/"> GPU Technology Conference (GTC) 2020. </td></tr>
       <tr valign="top"> <td>[01/2020] </td> <td> Invited talk at Toyota Technological Institute at Chicago (TTIC).</td></tr> -->
    <tr valign="top"> <td>[9/2022] </td> <td> We have finished one journal paper and it will be submitted soon </td></tr>
	<tr valign="top"> <td>[9/2022] </td> <td> One paper was submitted to  SDM 2023 on semi-supervised FL design for IoT devices.</td></tr>
	<tr valign="top"> <td>[7/2022] </td> <td> I was invited to serve as a PC Member for <a href="https://decaf-workshop.github.io/decaf-2022/">MICCAI Workshop on Distributed, Collaborative and Federated Learning (DeCaF)</a> </td></tr>
	<tr valign="top"> <td>[6/2022] </td> <td> Began my remote summer internship at The University of British Columbia  </td></tr>
    <tr valign="top"> <td>[4/2022] </td> <td> Made an online presentation on DASFAA2022, great experience!</td></tr>
	<tr valign="top"> <td>[1/2022] </td> <td> Our paper "Heterogeneous Federated Learning via Grouped Sequential-to-Parallel Training" was accepted by DASFAA2022, thanks to my co-authors!</td></tr>
    <tr valign="top"> <td>[10/2021] </td> <td> I was awarded WAC Scholarship(10 students among all students in UESTC) </td></tr>
    <tr valign="top"> <td>[6/2021] </td> <td> Began my remote summer internship at Peen State University, Park </td></tr>
    <tr valign="top"> <td>[4/2021] </td> <td> Submitted a national patent on the optimazation of federated learning</td></tr>
    <tr valign="top"> <td>[10/2020] </td> <td> I was awarded National Scholarship by Ministry of Education</td></tr>

	
  
<!--    <tr valign="top"> <td>[10/2022] </td> <td> One paper was accepted by <a href="https://ieeebibm.org/BIBM2022/"> BIBM 2022</a> on automated medical risk predictive modeling on EHR.</td></tr>-->
<!--    <tr valign="top"> <td>[10/2022] </td> <td> I was invited to serve as a reviewer for<a href="https://meta-learn.github.io/2022/"> NeurIPS 2022 Workshop MetaLearn</a>.</td></tr-->
<!--    <tr valign="top"> <td>[09/2022] </td> <td> I was invited to serve as a reviewer for<a href="https://logconference.org/"> Learning on Graphs</a>.</td></tr-->
<!--    <tr valign="top"> <td>[08/2022] </td> <td> One paper was accepted by <a href="https://icdm22.cse.usf.edu/"> ICDM 2022</a>.</td></tr-->
<!--    <tr valign="top"> <td>[08/2022] </td> <td> I was invited to serve as a PC Member for<a href="https://sites.google.com/view/fedgraph2022"> FedGraph 2022</a>.</td></tr-->
<!--    <tr valign="top"> <td>[07/2022] </td> <td> I was invited to serve as a PC Member for<a href="https://aaai.org/Conferences/AAAI-23/"> AAAI 2023</a>.</td></tr-->
<!--    <tr valign="top"> <td>[06/2022] </td> <td> One paper was accepted by <a href="https://2022.ecmlpkdd.org/"> ECML PKDD 2022</a> on COVID-19 vaccine side effect prediction using federated learning.</td></tr>-->
<!--    <tr valign="top"> <td>[03/2022] </td> <td> I will join  <a href="https://usa.visa.com/about-visa/visa-research.html"> Visa Research</a> as a machine learning research intern this summer.</td></tr>-->
<!--    <tr valign="top"> <td>[02/2022] </td> <td> I start to cooperate with <a href="https://www.sony.com/en/SonyInfo/research/"> Sony R&D Center</a> to work on federated learning in multi-domain applications.</td></tr>-->
<!--    <tr valign="top"> <td>[10/2021] </td> <td> One paper was accepted by <a href="https://bigdataieee.org/BigData2021/"> BigData 2021</a> on semi-supervised federated learning.</td></tr>-->
<!--    <tr valign="top"> <td>[04/2021] </td> <td> One paper was accepted by <a href="https://asian-chi.github.io/2021/"> Asian Chi 2021</a>. Thanks Dr. <a href="http://www.frankritter.com/ritter.html">Ritter</a>!</td></tr>-->
<!--    <tr valign="top"> <td>[04/2021] </td> <td> I will join <a href="https://www.iqvia.com/landing/analytics-center-of-excellence"> Analytics Center of Excellence, IQVIA </a> as a machine learning research intern in 2021 summer.</td></tr>-->
<!--    <tr valign="top"> <td>[03/2021] </td> <td> Received IST 2021 Travel Award.</td></tr>-->
<!--    <tr valign="top"> <td>[02/2021] </td> <td> I was invited to serve as a reviewer for<a href="https://2021.aclweb.org/"> ACL-IJCNLP 2021</a>.</td></tr>-->
<!--    &lt;!&ndash; <tr valign="top"> <td>[01/2021] </td> <td> I will join <a href="https://ailab.bytedance.com/"> ByteDance AI Lab</a> as a research intern in 2021 summer.</td></tr> &ndash;&gt;-->
<!--    <tr valign="top"> <td>[01/2021] </td> <td> One paper was accepted by <a href="https://ppai21.github.io/"> AAAI Workshop 2021</a> and selected as Spotlight Presentation.</td></tr>-->
<!--    <tr valign="top"> <td>[11/2020] </td> <td> I was invited to serve as a session chair at <a href="http://www.ieee-cybermatics.org/2020/cybermatics/"> IEEE Cybermatics Congress 2020</a>.</td></tr>-->
<!--    <tr valign="top"> <td>[10/2020] </td> <td> I was invited to give a talk at <a href="https://www.kennesaw.edu/"> Kennesaw State University</a>. Thank <a href="http://ksuweb.kennesaw.edu/~xxu6/">Dr. Xu</a> for host.</td></tr>-->
<!--    <tr valign="top"> <td>[10/2020] </td> <td> One paper was accepted by <a href="http://noisy-text.github.io/2020/"> W-NUT 2020</a>.</td></tr>-->
<!--    <tr valign="top"> <td>[09/2020] </td> <td> Served as an external reviewer for <a href="http://www.wsdm-conference.org/2021/"> WSDM 2021</a>.</td></tr>   -->
<!--	<tr valign="top"> <td>[08/2020] </td> <td> Served as an external reviewer for <a href="https://www.sigsac.org/ccs/CCS2020/"> CCS 2020</a>.</td></tr>-->
<!--	&lt;!&ndash; <tr valign="top"> <td>[08/2020] </td> <td> Top 25 in the finalist for <a href="https://outreach.didichuxing.com/competition/kddcup2020/"> KDD Cup 2020 LDR Competition</a>.</td></tr> &ndash;&gt;-->
<!--    <tr valign="top"> <td>[08/2020] </td> <td> I was invited to serve as a reviewer for <a href="https://www.ipccc.org/"> IEEE IPCCC 2020</a>.</td></tr>-->
<!--	<tr valign="top"> <td>[06/2020] </td> <td> I was invited to serve as a reviewer for <a href="http://wasa-conference.org/WASA2020/"> WASA 2020</a>.</td></tr>-->
<!--    <tr valign="top"> <td>[06/2020] </td> <td> I was invited to serve as a reviewer for <a href="https://www.emerald.com/insight/publication/issn/2398-6247"> Information Discovery and Delivery</a>.</td></tr>-->
<!--    <tr valign="top"> <td>[10/2019] </td> <td> I was invited to give a talk at the annual conference of Zhejiang University Alumni Association in North America.</td></tr>-->
<!--	<tr valign="top"> <td>[08/2019] </td> <td> I moved from Georgia to Pennsylvania. Bye old friends and hi new friends!-->
<!--	<tr valign="top"> <td>[03/2019] </td> <td> I received the 2019 Brahm Verma Graduate Leadership Award - Honorable Mention.-->
<!--	<tr valign="top"> <td>[10/2018] </td> <td> I was invited to give a talk at the Youth Scholar Forum hold by Georgia Tech and Association of Chinese Professionals.</td><tr>-->
	<!-- <tr valign="top"> <td>[10/2019] </td> <td> Invited speaker at the <a href="https://sites.google.com/site/iccv19clvllsmdc/home">ICCV 2019 Workshop on Closing the Loop Between Vision and Language</a>.</td></tr> -->
        <!--<tr valign="top"> <td>[04/2020] </td> <td> I accepted the invitation to serve as Program Committee Member of <a href = "http://www.icccn.org/"> ICCCN2020</a>. </td></tr> -->
        <!--<tr valign="top"> <td>[06/2019] </td> <td> Co-Organizer of the <a href="https://sites.google.com/site/iccv19clvllsmdc/home">workshop on Closing the Loop Between Vision and Language</a> at ICCV 2019. </td></tr> -->
        <!--<tr valign="top"> <td>[06/2019] </td> <td> Invited talk at Facebook AI.  </td></tr> <!-- "Language & Vision: Learning to Describe and Interact with the World". -->
        <!--<tr valign="top"> <td>[01/2019] </td> <td> Session Chair for AAAI 2019 (natural language processing). </td></tr> -->
</table>
</div>
</div>

<br><br>


<div id="publications">
    <h2> Selected Publications</h2>
    
    <!-- <h3>Preprint</h3> -->
    <table id="tbPublications" width="100%">
		<tr>
            <td width="100%">
            <p>
                <b>Heterogeneous Federated Learning via Grouped Sequential-to-Parallel Training</b><br>
                <b>Shenglai Zeng<b>*, Zonghang Li*, Hongfang Yu, Yihong He, Zenglin Xu, Dusit Niyato and Han Yu<br>
                <em><a href="https://www.dasfaa2022.org"> Oral, DASFAA 2022</a></em><br>
                [<a href="https://arxiv.org/pdf/2201.12976.pdf">Paper</a>] [<a href="https://drive.google.com/file/d/1Xj3TDDHZYRkQhTDpaDk193jbIciHRRby/view?usp=sharing">Video</a>] 
				
                
            </p>
            </td>
			
        </tr>
        <tr>
            <td width="100%">
            <p>
                <b>Knowledge-Enhanced Semi-Supervised Federated Learning for Aggregating Heterogeneous Lightweight Clients in IoT</b><br>
                Jiaqi Wang*, <b>Shenglai Zeng<b>*, Zewei Long, Yaqing Wang, Houping Xiao, Fenglong Ma<br>
                <em>Submitted to SDM 2023</em><br>
                <!-- [<a href="https://arxiv.org/pdf/2109.04533.pdf">Paper</a>] -->
                
            </p>
            </td>
        </tr>
		

		<tr>
            <td width="100%">
            <p>
                <b>HFedMS: Heterogeneous Federated Learning with Memorable Data Semantics in Industrial Metaverse</b><br>
                <b>Shenglai Zeng<b>*, Zonghang Li*, Hongfang Yu, Zhihao Zhang，Long Luo, Bo Li, Dusit Niyato<br>
                <em>Submitted to IEEE Transaction on Cloud Computing(JCR Q1)</em><br>
                [<a href="https://drive.google.com/file/d/1A6Ke24-o1eA2SO2ifkNRN7_2vsecUxTA/view?usp=sharing">Paper</a>]
				
                
            </p>
            </td>
        </tr>
		
       

        <!-- <tr>
            <td width="100%">
            <p>
                <b>Present situation and future prospect of renewable energy in China</b><br>
                Dahai Zhang, <b>Jiaqi Wang</b>, Yonggang Lin, Yulin Si, Can Huang, Jing Yang, Bin Huang, Wei Li<br>
                <em>Renewable and Sustainable Energy Reviews (TOP SCI，IF: 9.184)</em><br>
                [<a href="https://www.sciencedirect.com/science/article/abs/pii/S1364032117303416">Paper</a>]
            </p>
            </td>
        </tr> -->
    </table>
    
<!-- 


    
    <!-- <h3>2020</h3>
    <table id="tbPublications" width="100%">
        <tr>
            <td width="100%">
            <p>
                <b>Environment-agnostic Multitask Learning for Natural Language Grounded Navigation</b><br>
                <b>Xin Eric Wang</b>*, Vihan Jain*, Eugene Ie, William Wang, Zornitsa Kozareva, Sujith Ravi<br>
                <em>ECCV 2020</em><br>
                <font color="red">Ranking 1st on the CVDN leaderboard</font><br>
                [<a href="http://arxiv.org/abs/2003.00443">Paper</a>]
                [<a href="https://github.com/google-research/valan">Code</a>]
            </p>
            </td>
        </tr>
    
        <tr>
            <td width="306">
            <p>
                <b>Counterfactual Vision-and-Language Navigation via Adversarial Path Sampling</b><br>
                Tsu-Jui Fu, <b>Xin Eric Wang</b>, Matthew Peterson, Scott Grafton, Miguel Eckstein, William Yang Wang<br>
                <em>ECCV 2020</em><br>
                <font color="red">Spotlight presentation</font><br>
            [<a href="https://arxiv.org/abs/1911.07308">Paper</a>]
            </p>
            </td>
        </tr>
    
        <tr>
            <td width="306">
            <p>
                <b> REVERIE: Remote Embodied Visual Referring Expression in Real Indoor Environments</b><br>
                Yuankai Qi, Qi Wu, Peter Anderson, <b>Xin Wang</b>, William Yang Wang, Chunhua Shen, Anton van den Hengel<br>
                <em>CVPR 2020</em><br>
                <font color="red">Oral presentation</font><br>
            [<a href="https://arxiv.org/abs/1904.10151">Paper</a>]
            [<a href="https://github.com/YuankaiQi/REVERIE">Code</a>]
            </p>
            </td>
        </tr>
    
        <tr>
            <td width="306">
            <p>
                <b>Unsupervised Reinforcement Learning of Transferable Meta-Skills for Embodied Navigation</b><br>
                Juncheng Li, <b>Xin Wang</b>, Siliang Tang, Haizhou Shi, Fei Wu, Yueting Zhuang, William Yang Wang<br>
                <em>CVPR 2020</em><br>
            [<a href="https://arxiv.org/abs/1911.07450">Paper</a>]
            </p>
            </td>
        </tr>
    
        <tr>
            <td width="100%">
             <img src="indexpics/KGZS.png" width="285px" height="140px" style="box-shadow: 4px 4px 8px #888"> -->
           <!-- </td> -->
            <!-- <td> 
                <p>
                    <b>Vision-Language Navigation Policy Learning and Adaptation</b> <br>
                    <b>Xin Wang</b>, Qiuyuan Huang, Asli Celikyilmaz, Jianfeng Gao, Dinghan Shen, Yuan-Fang Wang, William Yang Wang, Lei Zhang <br>
                    <em> IEEE transactions on pattern analysis and machine intelligence (TPAMI), 2020</em> <br>
                    <font color="red">Journal version of the CVPR 2019 Best Student Paper</font>
                <p></p>
                <p> 
                   [<a href="https://arxiv.org/abs/2001.02332">paper</a>]
            </td>
        </tr>
    
        <tr>
            <td width="100%">
                <p>
                    <b> Generative Adversarial Zero-Shot Relational Learning for Knowledge Graphs</b> <br>
                    Pengda Qin, <b>Xin Wang</b>, Wenhu Chen, Chunyun Zhang, Weiran Xu, William Yang Wang<br>
                    <em> AAAI 2020</em> <br>
                    <font color="red">Oral presentation</font><br>
                [<a href="https://arxiv.org/abs/2001.02332">Paper</a>]
                [<a href="https://github.com/Panda0406/Zero-shot-knowledge-graph-relational-learning">Code</a>]
                </p>
            </td>
        </tr>
    </table>
    
    <h3>2019</h3>
    <table>
        <tr>
            <td width="100%">
                <p>
                    <b>TIGEr: Text-to-Image Grounding for Image Caption Evaluation </b> <br>
                    Ming Jiang, Qiuyuan Huang, Lei Zhang, <b>Xin Wang</b>, Pengchuan Zhang, Zhe Gan, Jana Diesner, Jianfeng Gao<br>
                    <em> EMNLP-IJCNLP 2019</em> <br>
                [<a href="https://arxiv.org/abs/1909.02050">Paper</a>]
                [<a href="https://github.com/SeleenaJM/CapEval">Code</a>]
                [<a href="javascript:void(0)" onclick="toggle_visibility('tiger_bibtex');">bibtex</a>]
                </p>
            </td>
        </tr>
    
        <tr>
            <td width="100%">
                <p>
                    <b>VATEX: A Large-Scale, High-Quality Multilingual Dataset for Video-and-Language Research</b> <br>
                    <b>Xin Wang</b>*, Jiawei Wu*, Junkun Chen, Lei Li, Yuan-Fang Wang, William Yang Wang<br>
                    <em> ICCV 2019</em> <br>
                    <font color="red">Oral presentation</font><br>
                [<a href="https://arxiv.org/abs/1904.03493">Paper</a>]
                [<a href="http://vatex-challenge.org">Website</a>]
                [<a href="javascript:void(0)" onclick="toggle_visibility('vatex_bibtex');">bibtex</a>]
                <div id='vatex_bibtex' style="display:none; font-size:small;">
                    @InProceedings{Wang_2019_ICCV, <br>
                    author = {Wang, Xin and Wu, Jiawei and Chen, Junkun and Li, Lei and Wang, Yuan-Fang and Wang, William Yang}, <br>
                    title = {VaTeX: A Large-Scale, High-Quality Multilingual Dataset for Video-and-Language Research},<br>
                    booktitle = {The IEEE International Conference on Computer Vision (ICCV)},<br>
                    month = {October},<br>
                    year = {2019}<br>
                    }
                </div>
                </p>
            </td>
        </tr>
    </table> --> 
  

<br><br>
<div id="experience">
<h2>Experience</h2>

<table id="tbPublications" width="100%">
<!---
    <tr>
        <td>
            <p><a href="https://ai.google/research/teams/language/">Google AI</a>, Mountain View, US</p>
            <p>&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp Research Intern, &nbsp Summer 2019</p>
            <p>&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp Mentors: 
                <a href="http://www.sravi.org/">Sujith Ravi</a>, 
                <a href="http://www.kozareva.com/">Zornitsa Kozareva</a>
            </p>
        </td>
        <td align="center">
            <p><img src="logos/google.jpeg" height=60px></p>
        </td>
    </tr>


    <tr>
        <td>
            <p><a href="https://research.fb.com/category/facebook-ai-research/">Facebook AI Research (FAIR)</a>, Menlo Park, US</p>
            <p>&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp Graduate Researcher, &nbsp Spring 2019</p>
            <p>&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp Mentors: 
                <a href="http://xinleic.xyz/">Xinlei Chen</a>,
                <a href="http://rohrbach.vision/">Marcus Rohrbach</a>,
                <a href="https://www.cc.gatech.edu/~dbatra/">Dhruv Batra</a>
                <!-- <a href="http://www.skamalas.com/">Yannis Kalantidis</a>
            </p> 
        </td>
        <td align="center">
            <p><img src="logos/FAIR.png" height=70px></p>
        </td>
    </tr>

    <tr>
        <td>
            <p><a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-redmond/">Microsoft Research AI</a>, Redmond, US</p>
            <p>&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp Research Intern, &nbsp Summer 2018</p>
            <p>&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp Mentors: 
                <a href="https://www.microsoft.com/en-us/research/people/leizhang/">Lei Zhang</a>, 
                <a href="https://www.microsoft.com/en-us/research/people/aslicel/">Asli Celikyilmaz</a>, 
                <a href="https://www.microsoft.com/en-us/research/people/jfgao/"> Jianfeng Gao</a>            
                <!-- <a href="http://www.qyhuang.site/">Qiuyuan Huang</a> 
            </p> 
        </td>
        <td align="center">
            <p><img src="logos/msr.jpg" height=60px></p>
        </td>
    </tr>

    <tr>
        <td>
            <p><a href="https://research.adobe.com/">Adobe Research</a>, San Francisco, US</p>
            <p>&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp Research Intern, &nbsp Summer 2017</p>
            <p>&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp Mentors: 
                <a href="https://www.linkedin.com/in/geoffoxholm/">Geoffrey Oxholm</a>, 
                <a href="http://www.oliverwang.info/"> Oliver Wang</a>, 
                <a href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a>, 
                <a href="https://research.adobe.com/person/michal-lukac/">Mike Lukac</a>
            </p> 
        </td>
        <td align="center">
            <p><img src="logos/adobe_2.jpg" height=80px></p>
        </td>
    </tr>

    <tr>
        <td>
            <p><a href="https://research.adobe.com/">Adobe Research</a>, San Francisco, US</p>
            <p>&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp Research Intern, &nbsp Summer 2016</p>
            <p>&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp Mentor: 
                <a href="https://www.linkedin.com/in/geoffoxholm/">Geoffrey Oxholm</a>
            </p> 
        </td>
        <td align="center">
            <p><img src="logos/adobe_2.jpg" height=80px></p> 
        </td>
    </tr>
-->
<tr>
    <td>
        <p> <a href = "https://www.ubc.ca">The University of British Columbia</a>, Vancouver, Canada </p>
        <p>&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp  Research Intern(MITACS program), &nbsp Summer 2022 </p>
        <p>&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp  Mentor: <a href = "https://xxlya.github.io/xiaoxiao/"> Xiaoxiao Li</a>
    </td>
    <td align="center">
        <p><img src="logos/ubc.png" height=80px></p>
    </td>
</tr> 

<tr>
    <td>
        <p> <a href = "https://www.psu.edu">The Pennsylvania State University</a>, Pennsylvania, US</p>
        <p>&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp  Research Intern, &nbsp Summer 2021 </p>
        <p>&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp  Mentor: <a href = "http://www.personal.psu.edu/ffm5105/index.html/"> Fenglong Ma</a>
    </td>
    <td align="center">
        <p><img src="logos/psu.png" height=80px></p>
    </td>
</tr>
    <tr>
    <td>
        <p> <a href = "https://www.uchicago.edu">The University of Chicago</a>, Chicago, US </p>
        <p>&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp  Online Intern, &nbsp Mar 2020-Mar 2021 </p>
        <p>&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp  Mentor: <a href = "https://www.shinan.info/"> Shinan Liu</a>
    </td>
    <td align="center">
        <p><img src="logos/uchicago.png" height=80px></p>
    </td>
</tr>

<tr>
    <td>
        <p> <a href = "http://www.uestc.edu.cn/"> University of Electronic Science and Technology of China</a>, Chengdu, China</p>
        <p>&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp  Research Assisant, &nbsp Oct 2020-Present  </p>
        <p>&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp  Mentor: <a href = "https://scholar.google.com/citations?user=GmEdMqwAAAAJ&hl=en"> Hongfang Yu</a>
    </td>
    <td align="center">
        <p><img src="logos/UESTC.png" height=80px></p>
    </td>
</tr>


</table>
</div>


<!---
<div id="projects and publications">
<h2> Selected Publications and Projects</h2>


<!--<h3>
<p>I have been publishing under Xin Eric Wang since 2020 (previously Xin Wang).</p>
</h3> 

<h3>Preprint</h3>
<table id="tbPublications" width="100%">
    <tr>
        <td width="100%">
        <p>
            <b>Multimodal Text Style Transfer for Outdoor Vision-and-Language Navigation</b><br>
            Wanrong Zhu, <b>Xin Wang</b>, Tsu-Jui Fu, An Yan, Pradyumna Narayana, Kazoo Sone, Sugato Basu, William Yang Wang<br>
            <em>Tech report 2020</em><br>
            <!-- <font color="red">Ranking 1st on the CVDN leaderboard</font><br> 
            [<a href="https://arxiv.org/abs/2007.00229">Paper</a>]
            <!-- [<a href="https://github.com/google-research/valan">Code</a>] 
        </p>
        </td>
    </tr>

    <tr>
        <td width="100%">
        <p>
            <b>Cross-Lingual Vision-Language Navigation</b><br>
            An Yan*, <b>Xin Wang</b>*, Jiangtao Feng, Lei Li, William Yang Wang<br>
            <em>Tech report 2020</em><br>
        [<a href="http://arxiv.org/abs/1910.11301">Paper</a>]
        </p>
        </td>
    </tr>

</table>

<h3>2020</h3>
<table id="tbPublications" width="100%">
    <tr>
        <td width="100%">
        <p>
            <b>Environment-agnostic Multitask Learning for Natural Language Grounded Navigation</b><br>
            <b>Xin Eric Wang</b>*, Vihan Jain*, Eugene Ie, William Wang, Zornitsa Kozareva, Sujith Ravi<br>
            <em>ECCV 2020</em><br>
            <font color="red">Ranking 1st on the CVDN leaderboard</font><br>
            [<a href="http://arxiv.org/abs/2003.00443">Paper</a>]
            [<a href="https://github.com/google-research/valan">Code</a>]
        </p>
        </td>
    </tr>

    <tr>
        <td width="306">
        <p>
            <b>Counterfactual Vision-and-Language Navigation via Adversarial Path Sampling</b><br>
            Tsu-Jui Fu, <b>Xin Eric Wang</b>, Matthew Peterson, Scott Grafton, Miguel Eckstein, William Yang Wang<br>
            <em>ECCV 2020</em><br>
            <font color="red">Spotlight presentation</font><br>
        [<a href="https://arxiv.org/abs/1911.07308">Paper</a>]
        </p>
        </td>
    </tr>

    <tr>
        <td width="306">
        <p>
            <b> REVERIE: Remote Embodied Visual Referring Expression in Real Indoor Environments</b><br>
            Yuankai Qi, Qi Wu, Peter Anderson, <b>Xin Wang</b>, William Yang Wang, Chunhua Shen, Anton van den Hengel<br>
            <em>CVPR 2020</em><br>
            <font color="red">Oral presentation</font><br>
        [<a href="https://arxiv.org/abs/1904.10151">Paper</a>]
        [<a href="https://github.com/YuankaiQi/REVERIE">Code</a>]
        </p>
        </td>
    </tr>

    <tr>
        <td width="306">
        <p>
            <b>Unsupervised Reinforcement Learning of Transferable Meta-Skills for Embodied Navigation</b><br>
            Juncheng Li, <b>Xin Wang</b>, Siliang Tang, Haizhou Shi, Fei Wu, Yueting Zhuang, William Yang Wang<br>
            <em>CVPR 2020</em><br>
        [<a href="https://arxiv.org/abs/1911.07450">Paper</a>]
        </p>
        </td>
    </tr>

    <tr>
        <td width="100%">
        <!-- <img src="indexpics/KGZS.png" width="285px" height="140px" style="box-shadow: 4px 4px 8px #888"> -->
        <!-- </td> -->
        <!-- <td> 
            <p>
                <b>Vision-Language Navigation Policy Learning and Adaptation</b> <br>
                <b>Xin Wang</b>, Qiuyuan Huang, Asli Celikyilmaz, Jianfeng Gao, Dinghan Shen, Yuan-Fang Wang, William Yang Wang, Lei Zhang <br>
                <em> IEEE transactions on pattern analysis and machine intelligence (TPAMI), 2020</em> <br>
                <font color="red">Journal version of the CVPR 2019 Best Student Paper</font>
            <p></p>
            <p> 
               [<a href="https://arxiv.org/abs/2001.02332">paper</a>]
        </td>
    </tr>

    <tr>
        <td width="100%">
            <p>
                <b> Generative Adversarial Zero-Shot Relational Learning for Knowledge Graphs</b> <br>
                Pengda Qin, <b>Xin Wang</b>, Wenhu Chen, Chunyun Zhang, Weiran Xu, William Yang Wang<br>
                <em> AAAI 2020</em> <br>
                <font color="red">Oral presentation</font><br>
            [<a href="https://arxiv.org/abs/2001.02332">Paper</a>]
            [<a href="https://github.com/Panda0406/Zero-shot-knowledge-graph-relational-learning">Code</a>]
            </p>
        </td>
    </tr>
</table>

<h3>2019</h3>
<table>
    <tr>
        <td width="100%">
            <p>
                <b>TIGEr: Text-to-Image Grounding for Image Caption Evaluation </b> <br>
                Ming Jiang, Qiuyuan Huang, Lei Zhang, <b>Xin Wang</b>, Pengchuan Zhang, Zhe Gan, Jana Diesner, Jianfeng Gao<br>
                <em> EMNLP-IJCNLP 2019</em> <br>
            [<a href="https://arxiv.org/abs/1909.02050">Paper</a>]
            [<a href="https://github.com/SeleenaJM/CapEval">Code</a>]
            [<a href="javascript:void(0)" onclick="toggle_visibility('tiger_bibtex');">bibtex</a>]
            </p>
        </td>
    </tr>

    <tr>
        <td width="100%">
            <p>
                <b>VATEX: A Large-Scale, High-Quality Multilingual Dataset for Video-and-Language Research</b> <br>
                <b>Xin Wang</b>*, Jiawei Wu*, Junkun Chen, Lei Li, Yuan-Fang Wang, William Yang Wang<br>
                <em> ICCV 2019</em> <br>
                <font color="red">Oral presentation</font><br>
            [<a href="https://arxiv.org/abs/1904.03493">Paper</a>]
            [<a href="http://vatex-challenge.org">Website</a>]
            [<a href="javascript:void(0)" onclick="toggle_visibility('vatex_bibtex');">bibtex</a>]
            <div id='vatex_bibtex' style="display:none; font-size:small;">
                @InProceedings{Wang_2019_ICCV, <br>
                author = {Wang, Xin and Wu, Jiawei and Chen, Junkun and Li, Lei and Wang, Yuan-Fang and Wang, William Yang}, <br>
                title = {VaTeX: A Large-Scale, High-Quality Multilingual Dataset for Video-and-Language Research},<br>
                booktitle = {The IEEE International Conference on Computer Vision (ICCV)},<br>
                month = {October},<br>
                year = {2019}<br>
                }
            </div>
            </p>
        </td>
    </tr>

    <tr>
        <td width="100%">
            <p>
                <b>Reinforced Cross-Modal Matching and Self-Supervised Imitation Learning for Vision-Language Navigation</b> <br>
                <b>Xin Wang</b>, Qiuyuan Huang, Asli Celikyilmaz, Jianfeng Gao, Dinghan Shen, Yuan-Fang Wang, William Yang Wang, Lei Zhang <br>
                <em>CVPR 2019</em> <br>
                <font color="red">Best Student Paper (1/5160=0.02%)</font><br>
            [<a href="https://arxiv.org/abs/1811.10092">Paper</a>]
            <!-- [<a>Code</a>] 
            [<a href="javascript:void(0)" onclick="toggle_visibility('rcm_bibtex');">bibtex</a>]
            <div id='rcm_bibtex' style="display:none; font-size:small;">
                @inproceedings{wang2019reinforced, <br>
                    title={Reinforced cross-modal matching and self-supervised imitation learning for vision-language navigation},<br>
                    author={Wang, Xin and Huang, Qiuyuan and Celikyilmaz, Asli and Gao, Jianfeng and Shen, Dinghan and Wang, Yuan-Fang and Yang Wang, William and Zhang, Lei},<br>
                    booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},<br>
                    pages={6629--6638},<br>
                    year={2019}<br>
                  }
            </div>
            </p>
        </td>
    </tr>

    <tr>
        <td width="100%">
            <p>
                <b>MAN: Moment Alignment Network for Natural Language Moment Retrieval via Iterative Graph Adjustment</b> <br>
                Da Zhang, Xiyang Dai, <b>Xin Wang</b>, Yuan-Fang Wang, Larry S. Davis <br>
                <em>CVPR 2019</em><br>
            [<a href="https://arxiv.org/abs/1812.00087">Paper</a>]
            <!-- [<a>Code</a>] 
            [<a href="javascript:void(0)" onclick="toggle_visibility('man_bibtex');">bibtex</a>]
            <div id='man_bibtex' style="display:none; font-size:small;">
                @inproceedings{zhang2019man,<br>
                    title={Man: Moment alignment network for natural language moment retrieval via iterative graph adjustment},<br>
                    author={Zhang, Da and Dai, Xiyang and Wang, Xin and Wang, Yuan-Fang and Davis, Larry S},<br>
                    booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},<br>
                    pages={1247--1257},<br>
                    year={2019}<br>
                    }
            </div>
            </p>
        </td>
    </tr>

    <tr>
        <td width="100%">
            <p>
                <b>Self-Supervised Dialogue Learning</b> <br>
                Jiawei Wu, <b>Xin Wang</b>, William Yang Wang<br>
                <em> ACL 2019</em> <br>
            [<a href="https://arxiv.org/abs/1907.00448">Paper</a>]
            [<a href="javascript:void(0)" onclick="toggle_visibility('ssn_bibtex');">bibtex</a>]
            <div id='ssn_bibtex' style="display:none; font-size:small;">
                  @article{wu2019self, <br>
                    title={Self-Supervised Dialogue Learning}, <br>
                    author={Wu, Jiawei and Wang, Xin and Wang, William Yang}, <br>
                    journal={arXiv preprint arXiv:1907.00448}, <br>
                    year={2019} <br>
                  }
            </div>
            </p>
        </td>
    </tr>

    <tr>
        <td width="306">
            <p>
                <b>Self-Supervised Learning for Contextualized Extractive Summarization</b> <br>
                Hong Wang, <b>Xin Wang</b>, Wenhan Xiong, Mo Yu, Xiaoxiao Guo, Shiyu Chang, William Yang Wang<br>
                <em> ACL 2019</em> <br>
                <!-- <font color="red">Oral presentation</font> 
            [<a href="https://arxiv.org/abs/1906.04466">Paper</a>]
            [<a href="https://github.com/hongwang600/Summarization">Code</a>]
            [<a href="javascript:void(0)" onclick="toggle_visibility('sum_bibtex');">bibtex</a>]
            <div id='sum_bibtex' style="display:none; font-size:small;">
                  @article{wang2019self, <br>
                    title={Self-Supervised Learning for Contextualized Extractive Summarization}, <br>
                    author={Wang, Hong and Wang, Xin and Xiong, Wenhan and Yu, Mo and Guo, Xiaoxiao and Chang, Shiyu and Wang, William Yang}, <br>
                    journal={arXiv preprint arXiv:1906.04466}, <br>
                    year={2019} <br>
                  }
            </div>
            </p>
        </td>
    </tr>

    <tr>
        <td width="306">
            <p>
                <b>Towards Generating Long and Coherent Text with Multi-Level Latent Variable Models</b> <br>
                Dinghan Shen, Asli Celikyilmaz, Yizhe Zhang, Liqun Chen, <b>Xin Wang</b>, Jianfeng Gao, Lawrence Carin <br>
                <em>ACL 2019</em> <br>
            [<a href="https://arxiv.org/abs/1902.00154">Paper</a>]
            [<a href="javascript:void(0)" onclick="toggle_visibility('long_bibtex');">bibtex</a>]
            <div id='long_bibtex' style="display:none; font-size:small;">
                @article{zhang2018man,<br>
                  title={MAN: Moment Alignment Network for Natural Language Moment Retrieval via Iterative Graph Adjustment},<br>
                  author={Zhang, Da and Dai, Xiyang and Wang, Xin and Wang, Yuan-Fang and Davis, Larry S.},<br>
                  journal={arXiv preprint arXiv:1812.00087},<br>
                  year={2018}<br>
                }
            </div>
            </p>
        </td>
    </tr>

    <tr>
        <td width="100%">
            <p>
                <b> Extract and Edit: An Alternative to Back-Translation for Unsupervised Neural Machine Translation</b> <br>
                Jiawei Wu, <b>Xin Wang</b>, William Yang Wang<br>
                <em> NAACL 2019</em> <br>
                <font color="red">Oral presentation</font><br>
            [<a href="https://arxiv.org/abs/1904.02331">Paper</a>]
            [<a href="javascript:void(0)" onclick="toggle_visibility('unmt_bibtex');">bibtex</a>]
            <div id='unmt_bibtex' style="display:none; font-size:small;">
                @article{wu2019extract,<br>
                  title={Extract and Edit: An Alternative to Back-Translation for Unsupervised Neural Machine Translation},<br>
                  author={Wu, Jiawei and Wang, Xin and Wang, William Yang},<br>
                  journal={arXiv preprint arXiv:1904.02331},<br>
                  year={2019}<br>
                }
            </div>
            </p>
        </td>
    </tr>

    <tr>
        <td width="100%">
            <p>
                <b> Learning to Compose Topic-Aware Mixture of Experts for Zero-Shot Video Captioning</b> <br>
                <b>Xin Wang</b>, Jiawei Wu, Da Zhang, Yu Su, William Yang Wang<br>
                <em> AAAI 2019</em> <br>
                <font color="red">Oral presentation</font><br>
            [<a href="https://arxiv.org/abs/1811.02765">Paper</a>]
            [<a href="https://github.com/eric-xw/Zero-Shot-Video-Captioning">Code</a>]
            [<a href="javascript:void(0)" onclick="toggle_visibility('aaai19_bibtex');">bibtex</a>]
            <div id='aaai19_bibtex' style="display:none; font-size:small;">
                @article{wang2019learning,<br>
                  title={Learning to Compose Topic-Aware Mixture of Experts for Zero-Shot Video Captioning},<br>
                  author={Wang, Xin and Wu, Jiawei and Zhang, Da and Su, Yu and Wang, William Yang},<br>
                  journal={arXiv preprint arXiv:1811.02765},<br>
                  year={2018}<br>
                }
            </div>
            </p>
        </td>
    </tr>

</table>

<h3>2018</h3>
<table id="tbPublications" width="100%">
    <tr>
        <td width="100%">
            <p>
                <b> Look Before You Leap: Bridging Model-Free and Model-Based Reinforcement Learning for Planned-Ahead Vision-and-Language Navigation</b> <br>
                <b>Xin Wang</b>*, Wenhan Xiong*, Hongmin Wang, William Yang Wang<br>
                <em> ECCV 2018</em><br>
            [<a href="https://arxiv.org/abs/1803.07729">Paper</a>]
            [<a href="javascript:void(0)" onclick="toggle_visibility('eccv18_bibtex');">bibtex</a>]
            <div id='eccv18_bibtex' style="display:none; font-size:small;">
                @InProceedings{wang2018look,<br>
                title={Look Before You Leap: Bridging Model-Free and Model-Based Reinforcement Learning for Planned-Ahead Vision-and-Language Navigation},<br>
                author={Wang, Xin and Xiong, Wenhan and Wang, Hongmin and Wang, William Yang},<br>
                booktitle = {The European Conference on Computer Vision (ECCV)}, <br>
                month = {September}, <br>
                year={2018}<br>
                }
            </div>
            </p>
        </td>
    </tr>

    <tr>
        <td width="100%">
            <p>
                <b>XL-NBT: A Cross-lingual Neural Belief Tracking Framework</b> <br>
                Wenhu Chen, Jianshu Chen, Yu Su, <b>Xin Wang</b>, Dong Yu, Xifeng Yan, William Yang Wang <br>
                <em> EMNLP 2018</em> <br>
            [<a href="https://arxiv.org/abs/1808.06244">Paper</a>]
            [<a href="https://github.com/wenhuchen/Cross-Lingual-NBT">Code</a>]
            [<a href="javascript:void(0)" onclick="toggle_visibility('emnlp18_bibtex');">bibtex</a>]
            <div id='emnlp18_bibtex' style="display:none; font-size:small;">
                @article{chen2018XLNBT, <br>
                title     = {XL-NBT: A Cross-lingual Neural Belief Tracking Framework}, <br>
                author    = {Chen, Wenhu and Chen, Jianshu and Su, Yu and  Wang, Xin and Yu, Dong and Yan, Xifeng and Wang, William Yang}, <br>
                booktitle = {arXiv preprint arXiv:1808.06244}, <br>
                year      = {2018} <br>
                } <br>
            </div>
            </p>
        </td>
    </tr>

    <tr>
        <td width="100%">
            <p>
                <b>No Metrics Are Perfect: Adversarial Reward Learning for Visual Storytelling</b> <br>
                <b>Xin Wang</b>*, Wenhu Chen*, Yuan-Fang Wang, William Yang Wang<br>
                <em> ACL 2018</em> <br>
                <font color="red">Oral presentation</font><br>
            [<a href="https://arxiv.org/abs/1804.09160">Paper</a>]
            [<a href="https://github.com/littlekobe/AREL">Code</a>]
            [<a href="https://vimeo.com/285801215">Video</a>]
            [<a href="slides/AREL.pptx">Slides (pptx)</a>]
            [<a href="slides/AREL.pdf">Slides (pdf)</a>]
            [<a href="javascript:void(0)" onclick="toggle_visibility('acl18_bibtex');">bibtex</a>]
            <div id='acl18_bibtex' style="display:none; font-size:small;">
                @InProceedings{wang2018AREL, <br>
                title     = {No Metrics Are Perfect: Adversarial Reward Learning for Visual Storytelling}, <br>
                author    = {Wang, Xin  and  Chen, Wenhu  and  Wang, Yuan-Fang and Wang, William Yang}, <br>
                booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, <br>
                year      = {2018} <br>
                } <br>
            </div>
            </p>
        </td>
    </tr>

    <tr>
        <td width="306">
            <p>
                <b>S3D: Single Shot multi-Span Detector via Fully 3D Convolutional Network</b> <br>
                Da Zhang, Xiyang Dai, <b>Xin Wang</b>, Yuan-Fang Wang<br>
                <em> BMVC 2018 </em> <br>
                <font color="red">Oral presentation</font><br>
            [<a href="https://arxiv.org/abs/1807.08069">Paper</a>]
            [<a href="https://github.com/dazhang-cv/S3D">Code</a>]
            [<a href="https://www.youtube.com/watch?v=wKIZh1XgndA&feature=youtu.be#t=0m55s">Video</a>]
            [<a href="slides/S3D_BMVC2018.pdf">Slides</a>]
            [<a href="javascript:void(0)" onclick="toggle_visibility('bmvc18_bibtex');">bibtex</a>]
            <div id='bmvc18_bibtex' style="display:none; font-size:small;">
                @inproceedings{zhang2018bmvc, <br>
                author = {Zhang, Da and Dai, Xiyang and Wang, Xin and Wang, Yuan-Fang}, <br>
                title = {S3D: Single Shot multi-Span Detector via Fully 3D Convolutional Network}, <br>
                booktitle = {Proceedings of the British Machine Vision Conference}, <br>
                year = {2018} <br>
                } <br>
            </div>
            </p>
        </td>
    </tr>

    <tr>
        <td width="306">
            <p>
                <b>Video Captioning via Hierarchical Reinforcement Learning</b> <br>
                <b>Xin Wang</b>, Wenhu Chen, Jiawei Wu, Yuan-Fang Wang, William Yang Wang <br>
                <em>CVPR 2018</em><br>
            [<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Video_Captioning_via_CVPR_2018_paper.pdf">Paper</a>]
            [<a href="http://openaccess.thecvf.com/content_cvpr_2018/Supplemental/0007-supp.pdf">Supp</a>]
            [<a href="data/CharadesCaptions.zip">Dataset</a>]
            <!-- [<a href="https://cloud.tencent.com/developer/article/1092810">Media (Chinese)</a>] 
            [<a href="javascript:void(0)" onclick="toggle_visibility('cvpr18_bibtex');">bibtex</a>]
            <div id='cvpr18_bibtex' style="display:none; font-size:small;">
                @InProceedings{wang2018video, <br>
                  title = {Video Captioning via Hierarchical Reinforcement Learning}, <br>
                  author = {Wang, Xin and Chen, Wenhu and Wu, Jiawei and Wang, Yuan-Fang and Wang, William Yang}, <br>
                  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, <br>
                  year = {2018} <br>
                }
            </div>
            </p>
        </td>
    </tr>

    <tr>
        <td width="306">
            <p>
                <b> Watch, Listen, and Describe: Globally and Locally Aligned Cross-Modal Attentions for Video Captioning </b> <br>
                <b>Xin Wang</b>, Yuan-Fang Wang, William Yang Wang <br>
                <em> NAACL-HLT 2018</em><br>
            [<a href="https://arxiv.org/abs/1804.05448">Paper</a>]
            [<a href="http://aclweb.org/anthology/N18-2125">Paper</a>]
            [<a href="javascript:void(0)" onclick="toggle_visibility('naacl18_bibtex');">bibtex</a>]
            <div id='naacl18_bibtex' style="display:none; font-size:small;">
                @InProceedings{wang2018watch, <br>
                author =  {Wang, Xin and Wang, Yuan-Fang and Wang, William Yang}, <br>
                title =   {Watch, Listen, and Describe: Globally and Locally Aligned Cross-Modal Attentions for Video Captioning}, <br>
                booktitle =   {Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)}, <br>
                year =    {2018} <br>
                }
            </div>
            </p>
        </td>
    </tr>
</table>

<h3>2017</h3>
<table>
    <tr>
        <td width="100%">
            <p>
                <b>Multimodal Transfer: A Hierarchical Deep Convolutional Neural Network for Fast Artistic Style Transfer</b> <br>
                <b>Xin Wang</b>, Geoffrey Oxholm, Da Zhang, Yuan-Fang Wang <br>
                <em>CVPR 2017</em><br>
            [<a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Wang_Multimodal_Transfer_A_CVPR_2017_paper.pdf">Paper</a>]
            [<a href="http://openaccess.thecvf.com/content_cvpr_2017/supplemental/Wang_Multimodal_Transfer_A_2017_CVPR_supplemental.pdf">Supp</a>]
            [<a href="data/cvpr17_images.zip">Images</a>]
            [<a href="https://github.com/fullfanta/multimodal_transfer">Code (Third-Party)</a>]
            <!-- [<a href="https://www.youtube.com/watch?v=IbVwC-eDjiE">Media</a>] 
            [<a href="javascript:void(0)" onclick="toggle_visibility('cvpr17_bibtex');">bibtex</a>]
            <div id='cvpr17_bibtex' style="display:none; font-size:small;">
                @InProceedings{Wang_2017_CVPR,<br>
                author = {Wang, Xin and Oxholm, Geoffrey and Zhang, Da and Wang, Yuan-Fang},<br>
                title = {Multimodal Transfer: A Hierarchical Deep Convolutional Neural Network for Fast Artistic Style Transfer},<br>
                booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},<br>
                month = {July},<br>
                year = {2017}<br>
                }
            </div>
            </p>
        </td>
    </tr>

    <tr>
        <td width="306">
        <p>
            <b>Deep Reinforcement Learning for Visual Object Tracking in Videos</b> <br>
            Da Zhang, Hamid  Maei, <b>Xin Wang</b>, Yuan-Fang Wang <br>
            <em>Tech report 2017</em><br>
        [<a href="https://arxiv.org/abs/1701.08936">Paper</a>]
        <!-- [<a href="papers/tracking_17.pdf">Paper</a>] 
        [<a href="javascript:void(0)" onclick="toggle_visibility('tracking_bibtex');">bibtex</a>]
        <div id='tracking_bibtex' style="display:none; font-size:small;">
            @article{zhang2017deep,<br>
              title={Deep Reinforcement Learning for Visual Object Tracking in Videos},<br>
              author={Zhang, Da and Maei, Hamid and Wang, Xin and Wang, Yuan-Fang},<br>
              journal={arXiv preprint arXiv:1701.08936},<br>
              year={2017}<br>
            }
        </div>
        </p>
        </td>
    </tr>
</table>

</div>
-->


<!-- </div> -->



<!--<div id='teaching'>
<h2>Teaching</h2>
<table id="tbTeaching" border="0" width="100%">
    <tbody>
        <tr>
            <td> 2018</td>
            <td>Winter</td>
            <td>Guest Lecturer</td>
            <td><a href="http://www.cs.ucsb.edu/~cs281b/">CS281b: Computer Vision and Image Analysis</a></td>
        </tr>
        <tr>
            <td> 2017</td>
            <td>Winter</td>
            <td>Teaching Assistant</td>
            <td><a href="http://william.cs.ucsb.edu/courses/index.php/Winter_2017_CS190I_Introduction_to_Natural_Language_Processing">CS190I: Introduction to Natual Language Processing</a></td>
        </tr>
        <tr>
            <td> 2016</td>
            <td>Fall</td>
            <td>Teaching Assistant</td>
            <td>CS180: Computer Graphics</td>
        </tr>
        <tr>
            <td> 2016</td>
            <td>Winter</td>
            <td>Teaching Assistant</td>
            <td>CS48: Computer Science Project</td>
        </tr>
        <tr>
            <td> 2015</td>
            <td>Fall</td>
            <td>Teaching Assistant</td>
            <td>CS16: Problem Solving with C++</td>
        </tr>
    </tbody>
</table>
</div> -->

<!-- <br><br> -->

<!-- <div id="awards">
<h2>Awards</h2>
<table style="border-spacing:2px">
	</tbody>
		<tr><td>Outstanding Graduates, Zhejiang University, 2015</td></tr>
		<tr><td>Excellent Bachelor Thesis, Zhejiang University, 2015</td></tr>
		<tr><td>CCF Outstanding Student Award (only 100 in China per year), China Computer Federation (CCF), 2014</td></tr>
		<tr><td>National Endeavor Scholarship, Ministry of Education, China, 2012 - 2014</td></tr>
		<tr><td>First Prize Outstanding Student Scholarship, Zhejiang University, 2013 - 2015</td></tr>
		<tr><td>Merit Student Award, Zhejiang University, 2013 - 2014</td></tr>
		<tr><td>Student Leadership Award, Zhejiang University, 2013</td></tr>
		<tr><td>Activity Achievement Scholarship, Zhejiang University, 2012</td></tr>
	</tbody>
</table>
</div>
 -->
<!-- </div> -->
<br><br>
<div id="projects">
    <h2>Selected Projects</h2>
    <table>

        <tr>
            <td width="306">
            <img src="indexpics/hfedms.png" width="285px" height="180px" style="box-shadow: 4px 4px 8px #888">
            </td>
            <td>
                <p>
                    <b>Heterogeneous Federated Learning with Memorable Data Semantics in Industrial Metaverse</b><br>
                    <em>Advised by professor Hongfang Yu</em><br>
            <p style="text-align:justify; text-justify:inter-ideograph;"> We present a high-performance and
efficient system named HFEDMS for incorporating practical FL
into Industrial Metaverse. HFEDMS reduces data heterogeneity
through dynamic grouping and training mode conversion (Dynamic Sequential-to-Parallel Training, STP). Then, it compensates
for the forgotten knowledge by fusing compressed historical
data semantics and calibrates classifier parameters (Semantic
Compression and Compensation, SCC). Finally, the network
parameters of feature extractor and classifier are synchronized
in different frequencies (Layer-wise Alternative Synchronization
Protocol, LASP) to reduce communication costs. HFEDMS improves the classification accuracy
by at least 6.4%  and saves both the overall runtime and transfer bytes by up to 98%.
            </p></br>

            </td>
        </tr>
    
    
        <tr>
            <td width="306">
            <img src="indexpics/FedGSP.png" width="285px" height="180px" style="box-shadow: 4px 4px 8px #888">
            </td>
            <td>
                <p>
                    <b>Heterogeneous Federated Learning via Grouped Sequential-to-Parallel Training</b><br>
                    <em>Advised by Professor Hongfang Yu</em><br>
            <p style="text-align:justify; text-justify:inter-ideograph;"> We propose a data heterogeneity-robust FL approach, FedGSP,
to address this challenge by leveraging on a novel concept of dynamic
Sequential-to-Parallel (STP) collaborative training. FedGSP assigns FL
clients to homogeneous groups to minimize the overall distribution divergence among groups, and increases the degree of parallelism by reas
signing more groups in each round. It is also incorporated with a novel
Inter-Cluster Grouping (ICG) algorithm to assist in group assignment,
which uses the centroid equivalence theorem to simplify the NP-hard
grouping problem to make it solvable. FedGSP improves the accuracy by 3.7% on average compared with seven
state-of-the-art approaches, and reduces the training time and commu-
nication overhead by more than 90%</p>

            </td>
        </tr>
    
        <tr>
                <td width="306">
                <img src="indexpics/FedEPS.png" width="285px" height="150px" style="box-shadow: 4px 4px 8px #888">
                </td>
                <td>
                    <p>
                        <b>Knowledge-Enhanced Semi-Supervised Federated Learning for
Aggregating Heterogeneous Lightweight Clients in IoT</b><br>
                        <em>Advised by Professor Fenglong Ma</a></em><br>
                        <p style="text-align:justify; text-justify:inter-ideograph;"> 
                                We propose a novel SemiFL framework named pFedKnow. pFedKnow generates lightweight personalized
client models via neural network pruning techniques to  reduce communication cost. Moreover, it incorporates pretrained large models as prior knowledge to guide the
aggregation of personalized client models and further
enhance the framework performance. Experiment results on both image and text datasets show that the
proposed pFedKnow outperforms state-of-the-art baselines as well as reducing considerable communication
cost.
                        </p>
                    <p> 
            
                    <!---[<a href="https://www.youtube.com/watch?v=IbVwC-eDjiE">Video</a>]--->
                    </p>
                </td>
            </tr>
        
        
        
        <tr>
            <td width="306">
            <img src="indexpics/M2M.png" width="285px" height="140px" style="box-shadow: 4px 4px 8px #888">
            </td>
            <td>
                <p>
                    <b>Reconstruct Motion with Audio Data from Microphone</b><br>
                    <em>In collabration with Shinan Liu
                    <p style="text-align:justify; text-justify:inter-ideograph;"> This work aims at using audio data collected by microphone to reconstruct user’s state of motion during recording time. We conduct physical experiment/mathematical modeling/machine learning to explore the correlation between sensing data(collected by gyroscope, accelerometer sensor, G-Sensor)and audio data(collected by microphone in the same panel). This work is led by Shinan Liu(PhD candidate of Uchicago). It’s my ﬁrst research project in my freshman year where I learn basic skills of scientiﬁc research. </p></br>
                <p>

                </p>
            </td>
        </tr>
    
        <!-- <tr>
            <td width="306">
            <img src="indexpics/pi.png" width="285px" height="140px" style="box-shadow: 4px 4px 8px #888">
            </td>
            <td>
                <p>
                    <b>Cyber-Physical System Security through Energy Auditing</b><br>
                    <p style="text-align:justify; text-justify:inter-ideograph;">Designed a real-time energy meter system to monitor the IoT system. This work is the foundation to detect and identify IoT cyber or physical attacks based on energy auditing. Voltage, current and power consumption can be read and transmitted wirelessly. </p></br>
                    
    
                <p> 
                </p>
            </td>
        </tr>	 -->
        
        
        
        
        
    </table>
    </div>

<br><br>

<div id="service">
<h2>Service</h2>
<!--<li>
<!--  <b>Organizer: </b> 
    <ul>
        <li><a href="https://alvr-workshop.github.io">Workshop on Advances in Language and Vision Research (ALVR)</a>, ACL 2020</li>
        <li><a href="https://languageandvision.github.io/">Workshop on Language & Vision with applications to Video Understanding</a>, CVPR 2020</li> 
        <li>Tutorial on Self-Supervised Deep Learning for NLP, AACL-IJCNLP 2020</li> 
        <li><a href="https://vatex.org">First VATEX Challenge for Multilingual Video Captioning</a></li>
        <li><a href="https://sites.google.com/site/iccv19clvllsmdc/home">Workshop on Closing the Loop Between Vision and Language (CLVL)</a>, ICCV 2019 </li>
    </ul>
</li>
<li>
    <b>Area Chair: </b> 
    <ul>
        <li><a href="https://2020.emnlp.org/">EMNLP 2020</a>&nbsp (Interpretability and Analysis of Models for NLP)</li>
    </ul>    
</li>
<li>
    <b>Session Chair: </b> 
    <ul>
    	<li><a href="https://aaai.org/Conferences/AAAI-19/">AAAI 2019</a> (Natural Language Processing) </li>
    </ul>
</li>
<li> -->

<li>
	
    <b>Conference Program Committee or Reviewer: </b>
    <a href="https://decaf-workshop.github.io/decaf-2022/">MICCAI Workshop on Distributed, Collaborative and Federated Learning (DeCaF-2022)</a> </td></tr>
<!--    <a href="https://aaai.org/Conferences/AAAI-23/"> AAAI 2023</a>, &nbsp-->
<!--    <a href="https://2021.aclweb.org/"> ACL-IJCNLP 2021</a>, &nbsp-->
<!--    <a href="http://noisy-text.github.io/2020/"> W-NUT 2020</a>, &nbsp-->
<!--    <a href="http://www.wsdm-conference.org/2021/"> WSDM 2021</a>, &nbsp-->
<!--    <a href="https://www.sigsac.org/ccs/CCS2020/"> CCS 2020</a>, &nbsp-->
<!--	<a href="https://www.ipccc.org/"> IEEE IPCCC 2020</a>, &nbsp-->
<!--	<a href="http://wasa-conference.org/WASA2020/"> WASA 2020</a> &nbsp-->
	
    <p style="margin-top:3px">
    </p>        
</li>

<!--<li>-->
<!--     <b>Journal Reviewer:</b>  -->
<!--	<a href="https://www.emerald.com/insight/publication/issn/2398-6247"> Information Discovery and Delivery</a> 	-->
<!--&lt;!&ndash; 	&nbsp<a href="http://link.springer.com/journal/11263">IJCV</a>-->
<!--	&nbsp<a href="https://www.computer.org/web/tvcg">TVCG</a>,-->
<!--	&nbsp<a href="http://www.signalprocessingsociety.org/publications/periodicals/image-processing/">TIP</a>,-->
<!--	&nbsp<a href="http://tcsvt.polito.it/">TCSVT</a>,-->
<!--	&nbsp<a href="http://link.springer.com/journal/371">TVC</a>,-->
<!--	&nbsp<a href="">CAD</a>,-->
<!--	&nbsp<a href="http://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6046">TMM</a> &ndash;&gt;-->
<!--	<p style="margin-top:3px">-->
<!--	</p>		-->
<!--</li>-->

<!--<li>-->
<!--    -->
<!--    -->
<!--    -->
<!--    <b>Session Chair: </b>-->
<!--    <a href="http://www.ieee-cybermatics.org/2020/cybermatics/"> IEEE Cybermatics Congress 2020</a> &nbsp	-->
<!--    <p style="margin-top:3px">-->
<!--    </p>        -->
<!--</li>-->

<!--<li>-->

<!--    <b>Judge: </b>-->
<!--        <a href="https://www.comap.com/undergraduate/contests/"> MCM 2019, 2020, 2021</a> &nbsp	-->
<!--    <p style="margin-top:3px">-->
<!--    </p>        -->
<!--</li>-->

<!-- <li>
	<b>Presentations:</b> 	<a href="http://www.iccv2013.org/">ICCV 2013</a>,
	&nbsp<a href="http://eccv2014.org/">ECCV 2014</a>,
	&nbsp <a href="http://pamitc.org/iccv15/">ICCV 2015</a>,
	&nbsp <a href="">Eurographics 2016</a>
	<p style="margin-top:3px">
	</p>		
</li> -->
<!-- </div> -->

<!--<br><br>-->

<!--<div id='Activities'>-->
<!--<h2>Activities</h2>-->
<!--<li> Product manager in a start-up team iDeal</li>-->
<!--&lt;!&ndash; <li> Top 25 in the finalist for <a href="https://outreach.didichuxing.com/competition/kddcup2020/"> KDD Cup 2020 LDR Competition</a></li> &ndash;&gt;-->
<!--<li> Best Team Award in <a href="https://evgrandprix.org/about/">EV Grand Prix Competition </a></li>-->
<!--<li> National Excellent Award in P&G Challenge</li>-->
<!--</div>-->


<!--<br><br>-->

<!--<div id='teaching'>-->
<!--<h2>Teaching</h2>-->
<!--    <tr>-->
<!--        <td>2022</td> -->
<!--        <td>Fall</td> &nbsp;&nbsp;&nbsp;&nbsp;-->
<!--        <td>Teaching Assistant</td> &nbsp;&nbsp;&nbsp;&nbsp;-->
<!--        <td>DS310: Machine Learning for Data Analytics</td>-->
<!--    </tr>-->
<!--    <br/>-->
<!--    <tr>-->
<!--        <td>2022</td> -->
<!--        <td>Spring</td> &nbsp;&nbsp;&nbsp;&nbsp;-->
<!--        <td>Teaching Assistant</td> &nbsp;&nbsp;&nbsp;&nbsp;-->
<!--        <td>DS310: Machine Learning for Data Analytics</td>-->
<!--    </tr>-->
<!--    <br/>-->
<!--    <tr>-->
<!--        <td>2021</td> -->
<!--        <td>Fall</td> &nbsp;&nbsp;&nbsp;&nbsp;-->
<!--        <td>Teaching Assistant</td> &nbsp;&nbsp;&nbsp;&nbsp;-->
<!--        <td>DS310: Machine Learning for Data Analytics</td>-->
<!--    </tr>-->
<!--    <br/>-->
<!--    <tr>-->
<!--        <td>2021</td> -->
<!--        <td>Spring</td> &nbsp;&nbsp;&nbsp;&nbsp;-->
<!--        <td>Teaching Assistant</td> &nbsp;&nbsp;&nbsp;&nbsp;-->
<!--        <td>DS300: Data Privacy and Security</td>-->
<!--    </tr>-->
<!--</div>-->


<!--<br><br>-->

<!--<div id='Personal'>-->
<!--<h2>Personal</h2>-->

<!--<li>-->
<!--    <b>Be kind and brave.</b>	-->
<!--    <p style="margin-top:3px"></p>        -->
<!--</li>-->
<!--<li>-->
<!--    <b>I am always <a href="./me.html"><b>having fun</b></a>.</b>	-->
<!--    <p style="margin-top:3px"></p>        -->
<!--</li>-->

<!--<li>-->
<!--    <b>Limited people are contributing to the world, while most ones are enjoying it.</b>	-->
<!--    <p style="margin-top:3px"></p>        -->
<!--</li>-->
<!--<li>-->
<!--    <b>I am writing a book to record the unforgettable and adventurous pieces in my life.</b>	-->
<!--    <p style="margin-top:3px"></p>        -->
<!--</li>-->
<!--<li>-->
<!--    <b>Co-founder of <a href="https://www.fablabs.io/labs/fabei"> Fablab Hangzhou</a>.</b> One of our targets is to help disabled children education via developing educational tools.-->
<!--    <p style="margin-top:3px"></p>        -->
<!--</li>-->
<!--</div>-->
	
	
	

<!-- <div id="footer"> -->
<!-- <p> -->
<!-- <center> -->
    <!-- <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=cbe090&w=300&d=aJJ1wUAUT3iAzbbM3So9Rj0rh8-XOWIxZRgWKqOvcDk&co=2d78ad&ct=ffffff&cmo=3acc3a&cmn=ff5353'></script> -->
    <!-- <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=FCxcl2_big3T4rMqnah_ewYaFcEKJ6hDiu0n41inbZA"></script> -->
    <!-- <br> -->
    <!-- &copy; Jiaqi (Jack) Wang -->
<!-- </center> -->
<!-- </p> -->

<!-- </div> -->
	

<div id="footer">
	<!-- <div id="footer-text"></div> -->
<p>
<center>
    <!-- <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=cbe090&w=300&d=aJJ1wUAUT3iAzbbM3So9Rj0rh8-XOWIxZRgWKqOvcDk&co=2d78ad&ct=ffffff&cmo=3acc3a&cmn=ff5353'></script> -->
    <!-- <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=FCxcl2_big3T4rMqnah_ewYaFcEKJ6hDiu0n41inbZA"></script>
    <br> -->
    &copy; Shenlai Zeng
</center>
</p>

</div>



</div>
</body>
</html>
